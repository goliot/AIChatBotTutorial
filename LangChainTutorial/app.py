import streamlit as st
import os
from pathlib import Path
import base64
import pygame

# í˜ì´ì§€ ì„¤ì • (ê°€ì¥ ë¨¼ì € ì‹¤í–‰ë˜ì–´ì•¼ í•¨)
st.set_page_config(page_title="ì°¨ìˆ˜ë¯¼ ì±—ë´‡", page_icon="ğŸ’¬")

from dotenv import load_dotenv
from openai import OpenAI, BadRequestError
import tempfile
import time
import hashlib
from datetime import datetime
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI()

# pygame ì´ˆê¸°í™”
pygame.mixer.init()

# í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
current_dir = Path(__file__).parent.absolute()

# ìŒì„± íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
speech_dir = current_dir / "speech_files"
os.makedirs(speech_dir, exist_ok=True)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "model" not in st.session_state:
    st.session_state.model = "gpt-4.1-nano"
if "temperature" not in st.session_state:
    st.session_state.temperature = 0.7
if "voice_model" not in st.session_state:
    st.session_state.voice_model = "tts-1"
if "voice_type" not in st.session_state:
    st.session_state.voice_type = "alloy"
if "voice_instructions" not in st.session_state:
    st.session_state.voice_instructions = ""

# ìŒì„± íŒŒì¼ ê²½ë¡œ ìƒì„± í•¨ìˆ˜
def get_speech_file_path(message_content):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    content_hash = hashlib.md5(message_content.encode()).hexdigest()[:8]
    return str(speech_dir / f"speech_{timestamp}_{content_hash}.mp3")

# ìŒì„± ìƒì„± ë° ì €ì¥ í•¨ìˆ˜
def generate_and_save_speech(text, file_path):
    try:
        with client.audio.speech.with_streaming_response.create(
            model=st.session_state.voice_model,
            voice=st.session_state.voice_type,
            input=text,
            instructions=st.session_state.voice_instructions if st.session_state.voice_instructions else None,
            response_format="mp3"
        ) as response:
            # íŒŒì¼ í™•ì¥ìê°€ ì—†ìœ¼ë©´ ì¶”ê°€
            if not file_path.endswith('.mp3'):
                file_path += '.mp3'
            response.stream_to_file(file_path)
            if not os.path.exists(file_path):
                raise Exception("ìŒì„± íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return file_path
    except Exception as e:
        st.error(f"ìŒì„± ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        raise

# ìŒì„± ì¬ìƒ í•¨ìˆ˜
def play_speech(file_path):
    try:
        pygame.mixer.music.load(file_path)
        pygame.mixer.music.play()
        while pygame.mixer.music.get_busy():
            time.sleep(0.1)
        pygame.mixer.music.unload()
    except Exception as e:
        st.error(f"ìŒì„± ì¬ìƒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

# íŒŒì¼ì„ base64ë¡œ ì¸ì½”ë”©í•˜ëŠ” í•¨ìˆ˜
def get_binary_file_downloader_html(file_path, file_label='File'):
    with open(file_path, 'rb') as f:
        data = f.read()
    b64 = base64.b64encode(data).decode()
    return f'<a href="data:audio/mp3;base64,{b64}" download="{os.path.basename(file_path)}">{file_label}</a>'

# PDF ë¬¸ì„œ ë¡œë“œ ë° ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
@st.cache_resource
def initialize_pdf_rag():
    # PDF íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
    pdf_dir = "pdfs"
    
    # ëª¨ë“  PDF ë¬¸ì„œë¥¼ ë¡œë“œ
    all_docs = []
    for filename in os.listdir(pdf_dir):
        if filename.endswith(".pdf"):
            file_path = os.path.join(pdf_dir, filename)
            loader = PyPDFLoader(file_path)
            docs = loader.load()
            all_docs.extend(docs)
    
    # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™”
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=200
    )
    
    # ë¬¸ì„œ ë¶„í• 
    texts = text_splitter.split_documents(all_docs)
    
    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embeddings = OpenAIEmbeddings()
    
    # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    vectorstore = FAISS.from_documents(texts, embeddings)
    
    # QA ì²´ì¸ ì´ˆê¸°í™”
    qa_chain = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(temperature=0),
        chain_type="stuff",
        retriever=vectorstore.as_retriever()
    )
    
    return qa_chain

# PDF RAG ì´ˆê¸°í™”
qa_chain = initialize_pdf_rag()

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.title("ì„¤ì • âš™ï¸")
    
    # AI ëª¨ë¸ ì„ íƒ
    model_options = {
        "GPT-4.1 Nano": "gpt-4.1-nano",
        "GPT-3.5 Turbo": "gpt-3.5-turbo",
        "GPT-4": "gpt-4",
        "GPT-4 Turbo": "gpt-4-turbo-preview"
    }
    selected_model = st.selectbox(
        "AI ëª¨ë¸ ì„ íƒ",
        options=list(model_options.keys()),
        index=list(model_options.keys()).index(next(k for k, v in model_options.items() if v == st.session_state.model))
    )
    st.session_state.model = model_options[selected_model]
    
    # Temperature ì„¤ì •
    st.session_state.temperature = st.slider(
        "ì°½ì˜ì„± (Temperature)",
        min_value=0.0,
        max_value=2.0,
        value=st.session_state.temperature,
        step=0.1,
        help="ê°’ì´ ë†’ì„ìˆ˜ë¡ ë” ì°½ì˜ì ì¸ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ë” ê²°ì •ì ì´ê³  ì¼ê´€ëœ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."
    )

    # ìŒì„± ì„¤ì • êµ¬ë¶„ì„ 
    st.divider()
    st.subheader("ìŒì„± ì„¤ì • ğŸ¤")

    # ìŒì„± ëª¨ë¸ ì„ íƒ
    voice_model_options = {
        "TTS-1": "tts-1",
        "TTS-1 HD": "tts-1-hd"
    }
    selected_voice_model = st.selectbox(
        "ìŒì„± ëª¨ë¸ ì„ íƒ",
        options=list(voice_model_options.keys()),
        index=list(voice_model_options.keys()).index(next(k for k, v in voice_model_options.items() if v == st.session_state.voice_model))
    )
    st.session_state.voice_model = voice_model_options[selected_voice_model]

    # ëª©ì†Œë¦¬ íƒ€ì… ì„ íƒ
    voice_type_options = {
        "Alloy": "alloy",
        "Echo": "echo",
        "Fable": "fable",
        "Onyx": "onyx",
        "Nova": "nova",
        "Shimmer": "shimmer"
    }
    selected_voice_type = st.selectbox(
        "ëª©ì†Œë¦¬ íƒ€ì… ì„ íƒ",
        options=list(voice_type_options.keys()),
        index=list(voice_type_options.keys()).index(next(k for k, v in voice_type_options.items() if v == st.session_state.voice_type))
    )
    st.session_state.voice_type = voice_type_options[selected_voice_type]

    # ìŒì„± ì§€ì‹œì‚¬í•­ ì„¤ì •
    st.subheader("ìŒì„± ì§€ì‹œì‚¬í•­")
    
    # ë¯¸ë¦¬ ì •ì˜ëœ ì§€ì‹œì‚¬í•­ ì˜µì…˜
    preset_instructions = {
        "ê¸°ë³¸": "",
        "ê±´ë°©ì§„": "ê±´ë°©ì§„ ëª©ì†Œë¦¬",
        "ì¹œê·¼í•œ": "ì¹œê·¼í•˜ê³  ë”°ëœ»í•œ ëª©ì†Œë¦¬",
        "ì§„ì§€í•œ": "ì§„ì§€í•˜ê³  ê¶Œìœ„ìˆëŠ” ëª©ì†Œë¦¬",
        "ì¬ë¯¸ìˆëŠ”": "ì¬ë¯¸ìˆê³  í™œê¸°ì°¬ ëª©ì†Œë¦¬",
        "ìŠ¬í”ˆ": "ìŠ¬í”„ê³  ê°ì •ì ì¸ ëª©ì†Œë¦¬",
        "ì¡°í­": "ì¡°í­ê°™ì€ ë§íˆ¬",
        "ìˆ ì·¨í•œ ì‚¬ëŒ": "ìˆ  ì·¨í•œ ëª©ì†Œë¦¬"
    }
    
    # ì§€ì‹œì‚¬í•­ ì„ íƒ ë°©ì‹
    instruction_mode = st.radio(
        "ì§€ì‹œì‚¬í•­ ì„¤ì • ë°©ì‹",
        ["ë¯¸ë¦¬ ì •ì˜ëœ ì˜µì…˜", "ì§ì ‘ ì…ë ¥"],
        horizontal=True
    )
    
    if instruction_mode == "ë¯¸ë¦¬ ì •ì˜ëœ ì˜µì…˜":
        selected_preset = st.selectbox(
            "ì§€ì‹œì‚¬í•­ ì„ íƒ",
            options=list(preset_instructions.keys()),
            index=list(preset_instructions.keys()).index(next(k for k, v in preset_instructions.items() if v == st.session_state.voice_instructions))
        )
        st.session_state.voice_instructions = preset_instructions[selected_preset]
    else:
        st.session_state.voice_instructions = st.text_input(
            "ì§ì ‘ ì§€ì‹œì‚¬í•­ ì…ë ¥",
            value=st.session_state.voice_instructions,
            placeholder="ì˜ˆ: ê±´ë°©ì§„ ëª©ì†Œë¦¬, ì¹œê·¼í•œ í†¤ìœ¼ë¡œ ë§í•˜ê¸° ë“±"
        )

# íƒ€ì´í‹€
st.title("AI ì±„íŒ…ë°© ğŸ¤–")

# ì˜¤ë””ì˜¤ HTML ìƒì„± í•¨ìˆ˜
def get_audio_html(audio_file):
    with open(audio_file, "rb") as f:
        audio_bytes = f.read()
    audio_b64 = base64.b64encode(audio_bytes).decode()
    audio_html = f"""
        <audio controls autoplay>
            <source src="data:audio/mp3;base64,{audio_b64}" type="audio/mp3">
            Your browser does not support the audio element.
        </audio>
    """
    return audio_html

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for i, message in enumerate(st.session_state.messages):
    with st.chat_message(message["role"]):
        st.write(message["content"])
        
        # AI ì‘ë‹µì—ë§Œ ìŒì„± ì¬ìƒ ë²„íŠ¼ ì¶”ê°€
        if message["role"] == "assistant":
            # ìŒì„± íŒŒì¼ ê²½ë¡œê°€ ì—†ìœ¼ë©´ ìƒì„±
            if "speech_file" not in message:
                try:
                    speech_file = get_speech_file_path(message["content"])
                    if not os.path.exists(speech_file):
                        speech_file = generate_and_save_speech(message["content"], speech_file)
                        st.success(f"ìŒì„± íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {speech_file}")
                    message["speech_file"] = speech_file
                except Exception as e:
                    st.error(f"ìŒì„± íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                    continue
            
            # ìŒì„± ì¬ìƒ
            try:
                if "speech_file" in message and os.path.exists(message["speech_file"]):
                    # HTML ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´ ì§ì ‘ ìƒì„±
                    with open(message["speech_file"], "rb") as f:
                        audio_bytes = f.read()
                    audio_b64 = base64.b64encode(audio_bytes).decode()
                    audio_html = f"""
                        <audio controls autoplay>
                            <source src="data:audio/mp3;base64,{audio_b64}" type="audio/mp3">
                            Your browser does not support the audio element.
                        </audio>
                    """
                    st.markdown(audio_html, unsafe_allow_html=True)
                else:
                    st.error("ìŒì„± íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ìƒì„±í•´ì£¼ì„¸ìš”.")
            except Exception as e:
                st.error(f"ìŒì„± ì¬ìƒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    # AI ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        try:
            # PDF RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±
            rag_response = qa_chain.invoke({"query": prompt})
            rag_answer = rag_response["result"]
            
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ì— ìŒì„± ì§€ì‹œì‚¬í•­ ë°˜ì˜
            system_message = {
                "role": "system",
                "content": f"ë‹¹ì‹ ì€ {st.session_state.voice_instructions}ë¡œ ëŒ€í™”í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ëª¨ë“  ì‘ë‹µì€ ì´ í†¤ê³¼ ìŠ¤íƒ€ì¼ì„ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤."
            } if st.session_state.voice_instructions else None

            # ë©”ì‹œì§€ ëª©ë¡ ì¤€ë¹„
            messages = [system_message] if system_message else []
            messages.extend([
                {"role": m["role"], "content": m["content"]}
                for m in st.session_state.messages
            ])
            
            # PDF RAG ë‹µë³€ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬í•¨
            messages.append({
                "role": "system",
                "content": f"ë‹¤ìŒì€ PDF ë¬¸ì„œì—ì„œ ì°¾ì€ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤: {rag_answer}"
            })

            # OpenAI API í˜¸ì¶œ
            response = client.chat.completions.create(
                model=st.session_state.model,
                messages=messages,
                temperature=st.session_state.temperature,
            )
            
            # AI ì‘ë‹µ í‘œì‹œ
            ai_response = response.choices[0].message.content
            st.write(ai_response)
            
            # ìŒì„± íŒŒì¼ ìƒì„± ë° ì €ì¥
            try:
                speech_file = get_speech_file_path(ai_response)
                generate_and_save_speech(ai_response, speech_file)
                st.success(f"ìŒì„± íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {speech_file}")
                
                # AI ì‘ë‹µì„ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": ai_response,
                    "speech_file": speech_file
                })
                
                # ìŒì„± ì¬ìƒ
                with open(speech_file, "rb") as f:
                    audio_bytes = f.read()
                st.audio(audio_bytes, format="audio/mp3")
            except Exception as e:
                st.error(f"ìŒì„± ìƒì„±/ì¬ìƒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            
        except BadRequestError as e:
            st.error(f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        except Exception as e:
            st.error(f"ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
